# -*- coding: utf-8 -*-
"""Submission_Recommender System_Muhammad Fatih Idlan

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SCnTJN2N2SYDXS2WW8vG0xbFYNpIXvbB

# **Recommendation System: Automating Book's Suggestion using Content-based and Collaborative Filtering**

By    : Muhammad Fatih Idlan (faiti.alfaqar@gmail.com)

## Project Overview
In todayâ€™s digital age, the volume of content and choices available to users across platforms is overwhelming. Recommender systems play an indispensable role in navigating this vast landscape, ensuring users discover relevant and engaging content without being inundated by irrelevant options. By personalizing user experiences, these systems have become a cornerstone in industries like e-commerce, entertainment, and education, boosting user satisfaction, retention, and revenue. This project delves into the development of a book recommendation system, leveraging content-base filtering and collaborative filtering techniques with RecommenderNet model to match users with books they are most likely to enjoy. Content-based filtering is an approach in recommendation systems that utilizes information from items or users to make recommendations. While, collaborative filtering, a widely used approach, relies on user-item interactions to uncover patterns and provide recommendations. Unfortunately, content-based filtering techniques rely on item metadata, meaning they require detailed item descriptions and well-structured user profiles to generate accurate recommendations. Albeit that, collaborative filtering come up as a complementer which can make unforeseen recommendations, meaning it might offer items that are relevant to the user even if the information is not in the user's profile [[ 1 ]](https://doi.org/10.1016/j.eij.2015.06.005). The system implementation is aimed to demonstrate how machine learning can be harnessed to create a seamless and personalized user experience in the context of literature discovery.

## Business Understanding
### Problem Statement
Starting with explanation from the background above, core problems that this project aims to solve are:

* How to develop a machine learning-based recommendation system for books using content-based and colaborative filtering?
* How are the results between those two techniques?

### Objectives
According to problem statement above, this project has several objectives too, that are:

* Develop a machine learning-based recommendation system for books using content-based and colaborative filtering
* Determining high performance model with variation of recommendation techniques

### Solution Approach
To achive the objectives, we need to perform several things such as:

* Using content-based and colaborative filtering compare the results between those two methods

## Import Package dan Libraries
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import kagglehub
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import precision_recall_fscore_support

"""## Data Loading"""

path = kagglehub.dataset_download("arashnic/book-recommendation-dataset")

print("Path to dataset files:", path)

path2book = f'{path}/Books.csv'
path2rating = f'{path}/Ratings.csv'
path2user = f'{path}/Users.csv'

"""Read the dataset using the pandas.read_csv function. Implement the following code."""

books = pd.read_csv(path2book)
ratings = pd.read_csv(path2rating)
users = pd.read_csv(path2user)

books.head()

ratings.head()

users.head()

print('Amount of book data:', len(books.ISBN.unique()))
print('Total book-rated by readers:', len(ratings.ISBN.unique()))
print('Total user:', len(users['User-ID'].unique()))

"""## Data Understanding

The dataset that used in this project is Book Recommendation Dataset, which can be accessed through kaggle [[ 2 ]](https://www.kaggle.com/datasets/arashnic/book-recommendation-dataset/data). This dataset consist of 3 csv files, Books.csv (271360 rows with 8 columns), Ratings.csv (1149780 rows with 3 columns), and Users.csv (27885 rows with 3 columns), also has 3 png file which irrelevant in this project. The explanation for each column can be seen below:

For Books.csv, the column are consist of:

* ISBN = International Standard Book Number of the books inside obtained from Amazon Web Services
* Book-Title = Title of the books obtained from Amazon Web Services
* Book-Author = The Author of the books obtained from Amazon Web Services
* Year-Of-Publication = Publication year of the books obtained from Amazon Web Services
* Publisher = The Publisher of the books obtained from Amazon Web Services
* Image-URL-S = URL for small sized Book's cover images point to the Amazon web site
* Image-URL-M = URL for medium sized Book's cover images point to the Amazon web site
* Image-URL-L = URL for large sized Book's cover images point to the Amazon web site

For Ratings.csv, the column are consist of:

* User-ID = Anonymized user identification in integers
* ISBN = International Standard Book Number of the books inside obtained from Amazon Web Services
* Book-Rating = Rating of the books, expressed on a scale from 1-10 (higher values denoting higher appreciation) in an explicit way, or expressed by 0 in implicit way

For Users.csv, the column are consist of:

* User-ID = Anonymized user identification in integers
* Location = Region of the reader in form of city, country
* Age = Age of the readers

### Univariate Exploratory Data Analysis

#### Book Variable
"""

books.info()

"""This code reveals that the "books.csv" file contains 271,360 records and comprises 8 columns: ISBN, Book-Title, Book-Author, Year-Of-Publication, Publisher, Image-URL-S, Image-URL-M, and Image-URL-L. Notably, the 'Year-Of-Publication' column is identified as having an object data type, whereas publication years are conventionally represented as integers. Hence, a correction to the data type will be carried out as an initial step.

There is an input error, so the text values will be removed before converting it to the integer data type.
"""

books[(books['Year-Of-Publication'] == 'DK Publishing Inc') | (books['Year-Of-Publication'] == 'Gallimard')]

"""Removing values in 'Year-Of-Publication' that are text."""

temp = (books['Year-Of-Publication'] == 'DK Publishing Inc') | (books['Year-Of-Publication'] == 'Gallimard')
books = books.drop(books[temp].index)
books[(books['Year-Of-Publication'] == 'DK Publishing Inc') | (books['Year-Of-Publication'] == 'Gallimard')]

"""Changing the data type of 'Year-Of-Publication'."""

books['Year-Of-Publication'] = books['Year-Of-Publication'].astype(int)
print(books.dtypes)

"""The data type of the 'Year-Of-Publication' column has now been successfully converted to integer. The next step involves eliminating variables that are not relevant to the model development process. In a content-based filtering recommendation system, recommendations are generated based on books with the same title as those read by the user, while also considering the author's information. Consequently, details such as image sizes are unnecessary, allowing the removal of the 'Image-URL-S,' 'Image-URL-M,' and 'Image-URL-L' columns from the dataset.







"""

# Removing Image-URL column of all sizes
books.drop(labels=['Image-URL-S', 'Image-URL-M', 'Image-URL-L'], axis=1, inplace=True)

books.head()

print("Number of Book ISBN numbers:", len(books['ISBN'].unique()))
print("Number of book titles:", len(books['Book-Title'].unique()))
print('Number of book authors:', len(books['Book-Author'].unique()))
print('Number of Publication Years:', len(books['Year-Of-Publication'].unique()))
print('Number of publisher names:', len(books['Publisher'].unique()))

"""The output reveals the counts for each variable, showing that the dataset contains 242,135 unique book titles but 271,357 ISBN. This discrepancy suggests the presence of books without ISBN, as each book is expected to have a distinct ISBN. To address this, the dataset will be filtered to retain only entries with unique ISBN."""

# Grouping Book-Author' and count the number of books written by each author
author_counts = books.groupby('Book-Author')['Book-Title'].count()

# Sort authors in descending order
sorted_authors = author_counts.sort_values(ascending=False)

# Select the top 10 authors
top_10_authors = sorted_authors.head(10)

# The plot of the top 10 authors and the books written by the authors, then calculated using a bar plot
plt.figure(figsize=(12, 6))
top_10_authors.plot(kind='bar')
plt.xlabel('Author Name')
plt.ylabel('Number of Books')
plt.title('Top 10 Authors by Number of Books')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

"""The data indicates that Agatha Christie is the most prolific author, with over 600 books to her name. This also highlights that the dataset includes multiple authors who have contributed more than one book title.

#### Ratings Variable
"""

ratings.head()

ratings.info()

"""The output shows a total of 1,149,780 records across three columns: User-ID, representing the unique identifier for each anonymous user who provided ratings; ISBN, the unique identifier for each book; and Book-Rating, indicating the score assigned to the book by a user. To view the entry counts for each variable, execute the following code."""

print('Number of User-IDs:', len(ratings['User-ID'].unique()))
print('Number of books based on ISBN:', len(ratings['ISBN'].unique()))

print('Number of book ratings:')
sorted_ratings = ratings['Book-Rating'].value_counts().sort_index()
pd.DataFrame({'Book-Rating': sorted_ratings.index, 'Sum': sorted_ratings.values})

"""The output reveals that 105,283 users have provided ratings for books. A total of 340,556 unique books, identified by their ISBNs, received ratings, which range from 0 (the lowest score) to 10 (the highest score).

The "ratings" dataset contains a total of 1,149,780 rows, representing a significant volume of data. Since this dataset will be utilized for collaborative filtering in model development, only a subset will be selected to optimize memory usage during training. Specifically, the first 5,000 entries (excluding entry 5000) will be extracted for this purpose. This subset will serve as the basis for building a collaborative filtering model, which relies on user rating data to generate book title recommendations for other users. To streamline the process and minimize confusion with similar features, the dataset is renamed to "df_rating."
"""

df_rating = ratings[:20000]
df_rating

"""#### Users Variable"""

users.head()

users.info()

"""The dataset contains 278,858 records and includes three variables: User-ID, a unique identifier for anonymous users; Location, representing the users' geographical information; and Age, indicating the users' age. It is observed that some users have missing age information. While user data can be valuable for developing recommendation systems based on demographic or social factors, this study will not incorporate user data into the model. Instead, the focus will be on utilizing the "books" and "ratings" datasets for model development.

## Data Preprocessing

### Merging DataFrame and Determining the Total Number of Ratings
"""

# Merging dataframe ratings with books based on ISBN values
books = pd.merge(ratings, books, on='ISBN', how='left')
books

books.groupby('ISBN').sum()

"""## Data Preparation for Model Development with Content-based Filtering

### Handling Missing Value
"""

# Checking missing value using isnull() function
books.isnull().sum()

"""Several features in the dataset contain a significant number of missing values. However, the User-ID, ISBN, and Book-Rating features have no missing entries. The feature with the highest count of missing values is 'Publisher,' with 118,650 missing entries. Considering the dataset's total size of 1,149,780 records, this represents a relatively small and manageable proportion. As a result, the missing values will be removed, and a cleaned version of the dataset will be created under the name 'all_books_clean.'"""

all_books_clean = books.dropna()
all_books_clean

all_books_clean.isnull().sum()

"""### Standardizing Book Types Based on ISBN"""

fix_books = all_books_clean.sort_values('ISBN', ascending=True)
fix_books

len(fix_books['ISBN'].unique())

len(fix_books['Book-Title'].unique())

"""The information provided reveals a mismatch between the number of ISBNs and book titles, suggesting that certain ISBNs are linked to multiple titles. To address this and prepare the dataset for modeling, it is essential to ensure data uniqueness. This will involve eliminating duplicate entries in the 'ISBN' column. The cleaned dataset will then be stored in a new variable named 'preparation.' You can use the following code to execute this process."""

preparation = fix_books.drop_duplicates('ISBN')
preparation

# convert the 'ISBN' data series into list form
isbn_id = preparation['ISBN'].tolist()

# convert the 'Book-Title' data series into list form
book_title = preparation['Book-Title'].tolist()

# convert the 'Book-Author' data series into list form
book_author = preparation['Book-Author'].tolist()

# convert the 'Year-Of-Publication' data series into list form
year_of_publication = preparation['Year-Of-Publication'].tolist()

# convert the 'Publisher' data series into list form
publisher = preparation['Publisher'].tolist()

print(len(isbn_id))
print(len(book_title))
print(len(book_author))
print(len(year_of_publication))
print(len(publisher))

"""From the output provided, it is clear that the dataset has been refined, ensuring that the number of entries for ISBN, book title, author, year of publication, and publisher are now consistent and unique. Following the removal of duplicate entries, the dataset now consists of 270,144 rows. The subsequent task is to construct a dictionary that defines key-value pairs for the isbn_id, book_title, book_author, year_of_publication, and publisher fields. This prepared data will serve as the foundation for developing the content-based filtering recommendation system."""

books_new = pd.DataFrame({
    'isbn': isbn_id,
    'book_title': book_title,
    'book_author': book_author,
    'year_of_publication': year_of_publication,
    'publisher': publisher

})

books_new

books_new = books_new[:20000]

books_new

"""## Data Preparation for Model Development with Collaborative Filtering

For the collaborative filtering model, the dataset will be divided into training and validation sets during the training process. Prior to splitting, the data must undergo preparation. This involves transforming the rating data into a numerical matrix to streamline the model's ability to interpret and learn from it effectively. As part of this stage, several preprocessing steps will be applied: encoding the 'User-ID' and 'ISBN' features into integer indices, mapping 'User-ID' and 'ISBN' to their respective dataframes, and verifying critical data attributes such as the total number of users and books. Additionally, the rating values will be converted to float format to ensure compatibility during the model training phase.
"""

# convert User-ID to a list without matching values
user_ids = df_rating['User-ID'].unique().tolist()
print('list userIDs: ', user_ids)

# perform User-ID encoding
user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}
print('encoded userID: ', user_to_user_encoded)

# carry out the process of encoding numbers into User-ID
user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}
print('encoded number to userID: ', user_encoded_to_user)

# convert ISBNs to a list without matching values
isbn_id = df_rating['ISBN'].unique().tolist()

# perform ISBN encoding
isbn_to_isbn_encoded = {x: i for i, x in enumerate(isbn_id)}

# carry out the process of encoding numbers to ISBN
isbn_encoded_to_isbn = {i: x for i, x in enumerate(isbn_id)}

# Disable the SettingWithCopyWarning warning
pd.options.mode.chained_assignment = None # "warn" or "raise" to turn it back on

# Mapping User-ID to user dataframe
df_rating['user'] = df_rating['User-ID'].map(user_to_user_encoded)

# Mapping ISBN to book title dataframe
df_rating['book_title'] = df_rating['ISBN'].map(isbn_to_isbn_encoded)

# get the number of users
num_users = len(user_to_user_encoded)
print(num_users)

# get the number of book titles
num_book_title = len(isbn_to_isbn_encoded)
print(num_book_title)

# convert the rating to a float value
df_rating['Book-Rating'] = df_rating['Book-Rating'].values.astype(np.float32)

# minimum rating value
min_rating = min(df_rating['Book-Rating'])

# maximum rating value
max_rating = max(df_rating['Book-Rating'])

print('Number of Users: {}, Number of Books: {}, Min Rating: {}, Max Rating: {}'.format(
     num_users, num_book_title, min_rating, max_rating
))

"""## Model Development with Content-based Filtering

The model uses Content-Based Filtering to recommend items based on user preferences and item features. TF-IDF Vectorizer identifies key features of book titles, focusing on authors, while cosine similarity measures how closely books are related. Sklearn's tfidfvectorizer() and cosine_similarity() functions are applied to implement these steps.
"""

data = books_new
data.sample(5)

"""### TF-IDF Vectorizer"""

# Initialize TfidfVectorizer
TF = TfidfVectorizer()

# Perform IDF calculations on book_author data
TF.fit(data['book_author'])

# Mapping array from integer index features to name features
TF.get_feature_names_out()

# Performs a fit and then transforms it into matrix form
tfidf_matrix = TF.fit_transform(data['book_author'])

# View the tfidf matrix size
tfidf_matrix.shape

tfidf_matrix.todense()

pd.DataFrame(
    tfidf_matrix.todense(),
    columns=TF.get_feature_names_out(),
    index=data.book_title
).sample(15, axis=1).sample(10, axis=0)

"""The TF-IDF matrix successfully captures key feature representations for each book title using the tfidfvectorizer function. Since only a sample dataset is displayed, the full matrix is not shown. From 20,000 entries, a random selection of 10 book titles (vertical axis) and 15 author names (horizontal axis) is presented.

### Cosine Similarity
"""

# Calculating cosine similarity on the tf-idf matrix
cosine_sim = cosine_similarity(tfidf_matrix)
cosine_sim

"""At this stage, cosine similarity is calculated for the tfidf_matrix dataframe generated earlier. By utilizing the cosine_similarity function from the sklearn library, similarity scores between book titles are derived. The code outputs a similarity matrix in an array format."""

# Create a dataframe from the cosine_sim variable with rows and columns in the form of book titles
cosine_sim_df = pd.DataFrame(cosine_sim, index=data['book_title'], columns=data['book_title'])
print('Shape:', cosine_sim_df.shape)

# View the similarity matrix for each book title
cosine_sim_df.sample(5, axis=1).sample(10, axis=0)

"""Cosine similarity successfully identifies relationships between book titles, producing a 20,000 x 20,000 similarity matrix. This indicates the system has analyzed similarity levels for 20,000 titles. For clarity, only a subsetâ€”10 titles on the vertical axis and 5 on the horizontalâ€”is shown. Using this similarity data, the system can recommend books similar to those a user has read or purchased.

### Getting Recommendations
"""

def book_recommendation(book_title, similarity_data=cosine_sim_df, items=data[['book_title', 'book_author']], k=10):
     # Retrieve data by using argpartition to partition indirectly along a given axis
     # Dataframe converted to numpy
     # Range(start, stop, step)
     index = similarity_data.loc[:,book_title].to_numpy().argpartition(range(-1, -k, -1))

     # Retrieve data with the greatest similarity from the existing index
     closest = similarity_data.columns[index[-1:-(k+2):-1]]

     # Drop book_title so that the name of the book you are looking for does not appear in the recommendation list
     closest = closest.drop(book_title, errors='ignore')

     return pd.DataFrame(closest).merge(items).head(k)

book_title_test = "Joyful Noise (rpkg) : Poems for Two Voices" # book title example

data[data.book_title.eq(book_title_test)]

# Get recommendations for similar book titles
book_recommendation(book_title_test)

"""The system successfully recommends the top 5 book titles under the author category "Thomas Merton".

## Model Development with Collaborative Filtering

This project applies collaborative filtering to recommend books based on user ratings, assuming similar preferences predict similar choices. The model uses embeddings for users and books, calculates match scores with a sigmoid function, and incorporates user/book biases. Adapted from a Keras tutorial [[ 3 ]](https://keras.io/examples/structured_data/collaborative_filtering_movielens/), it uses Binary Crossentropy for loss, Adam for optimization, and RMSE for evaluation.

### Splitting Data for Training and Validation
"""

df_rating = df_rating.sample(frac=1, random_state=37)
df_rating

"""The data is split into 90% training and 10% validation. Before this, user and book title data are mapped to single values, and ratings are scaled to a 0-1 range for easier training."""

# create a variable x to match user data and book title into one value
x = df_rating[['user', 'book_title']].values

# create a y variable to create a rating of the results
y = df_rating['Book-Rating'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values

# divide into 90% train data and 10% validation data

train_indices = int(0.9 * df_rating.shape[0])
x_train, x_val, y_train, y_val = (
     x[:train_indices],
     x[train_indices:],
     y[:train_indices],
     y[train_indices:]
)

print(x, y)

"""### Training Process

During training, the model calculates match scores between users and book titles using embeddings. User and book embeddings are multiplied via dot product, with optional bias added. Scores are scaled to [0, 1] using a sigmoid activation function. The RecommenderNet class, built using the Keras Model class and adapted from a Keras tutorial, is used for this process.
"""

class RecommenderNet(tf.keras.Model):

     # function initialization
     def __init__(self, num_users, num_book_title, embedding_size, dropout_rate=0.2, **kwargs):
         super(RecommenderNet, self).__init__(**kwargs)
         self.num_users = num_users
         self.num_book_title = num_book_title
         self. embedding_size = embedding_size
         self.dropout_rate = dropout_rate

         self.user_embedding = layers.Embedding( # user embedding layer
             num_users,
             embedding_size,
             embeddings_initializer = 'he_normal',
             embeddings_regularizer =keras.regularizers.l2(1e-6)
         )
         self.user_bias = layers.Embedding(num_users, 1) # layer embedding user bias

         self.book_title_embedding = layers.Embedding( # book_title embedding layer
             num_book_title,
             embedding_size,
             embeddings_initializer = 'he_normal',
             embeddings_regularizer =keras.regularizers.l2(1e-6)
         )
         self.book_title_bias = layers.Embedding(num_book_title, 1) # layer embedding book_title

         self.dropout = layers.Dropout(rate=dropout_rate)

     def call(self, inputs):
         user_vector = self.user_embedding(inputs[:, 0]) # call embedding layer 1
         user_vector = self.dropout(user_vector)
         user_bias = self.user_bias(inputs[:, 0]) # call embedding layer 2

         book_title_vector = self.book_title_embedding(inputs[:, 1]) # call embedding layer 3
         book_title_vector = self.dropout(book_title_vector)
         book_title_bias = self.book_title_bias(inputs[:, 1]) # call embedding layer 4

         dot_user_book_title = tf.tensordot(user_vector, book_title_vector, 2) # dot product multiplication

         x = dot_user_book_title + user_bias + book_title_bias

         return tf.nn.sigmoid(x) # activate sigmoid

model = RecommenderNet(num_users, num_book_title, 50) # initialize model

# model compile
model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = keras.optimizers.Adam(learning_rate=1e-4),
    metrics = [tf.keras.metrics.RootMeanSquaredError()]
)

# start the training process

history = model.fit(
    x = x_train,
    y = y_train,
    batch_size = 16,
    epochs = 50,
    validation_data = (x_val, y_val),
)

"""Based on the results of the model training process, satisfactory results are obtained, and the model converges at around 50 epochs. From this process, a Root Mean Squared Error (RMSE) value of approximately 0.2924 and an RMSE on the validation data of 0.3389 are obtained. These values are quite good for a recommendation system. To see the results of the model development, the next step is to get book title recommendations based on the developed model.

### Getting Book Title Recommendations

To obtain book title recommendations, first, a random sample of users is taken, and the variable book_not_read is defined, which is a list of books that the user has never read or purchased. This variable book_not_read will be the book titles recommended by the system.

The book_not_visited variable is obtained by using the bitwise NOT operator (~) on the book_read_by_user variable.
"""

book_df = books_new

# take a sample of users
user_id = df_rating['User-ID'].sample(1).iloc[0]
book_readed_by_user = df_rating[df_rating['User-ID'] == user_id]

# create variable book_not_readed
book_not_readed = book_df[~book_df['isbn'].isin(book_readed_by_user['ISBN'].values)]['isbn']
book_not_readed = list(
    set(book_not_readed).intersection(set(isbn_to_isbn_encoded.keys()))
)

book_not_readed = [[isbn_to_isbn_encoded.get(x)] for x in book_not_readed]
user_encoder = user_to_user_encoded.get(user_id)
user_book_array = np.hstack(
    ([[user_encoder]] * len(book_not_readed), book_not_readed)
)

ratings_model = model.predict(user_book_array).flatten()

top_ratings_indices = ratings_model.argsort()[-10:][::-1]

recommended_book_ids = [
    isbn_encoded_to_isbn.get(book_not_readed[x][0]) for x in top_ratings_indices
]

top_book_user = (
    book_readed_by_user.sort_values(
        by='Book-Rating',
        ascending=False
    )
    .head(10)['ISBN'].values
)

book_df_rows = book_df[book_df['isbn'].isin(top_book_user)]

# Displays book recommendations in DataFrame form
book_df_rows_data = []
for row in book_df_rows.itertuples():
    book_df_rows_data.append([row.book_title, row.book_author])

recommended_book = book_df[book_df['isbn'].isin(recommended_book_ids)]

recommended_book_data = []
for row in recommended_book.itertuples():
    recommended_book_data.append([row.book_title, row.book_author])

# Create a DataFrame for output
output_columns = ['Book Title', 'Book Author']
df_book_readed_by_user = pd.DataFrame(book_df_rows_data, columns=output_columns)
df_recommended_books = pd.DataFrame(recommended_book_data, columns=output_columns)

# Displays recommendation results in DataFrame form
print("Showing recommendation for users: {}".format(user_id))
print("===" * 9)
print("Book with high ratings from user")
print("----" * 8)
print(df_book_readed_by_user)
print("----" * 8)
print("Top 10 books recommendation")
print("----" * 8)
df_recommended_books

"""Recommendations have been successfully generated for user ID 3363. The output compares "Books highly rated by the user" with the "Top 10 recommended books." Notably, some suggested titles include authors matching the user's preferences, with one book also being the user's highest-rated.

## Model Evaluation with Content-based Filtering

The model's performance is evaluated using Precision, Recall, and F1-Score. Precision measures relevant recommendations, Recall assesses identified relevant items, and F1-Score balances both. Ground truth labels, created from cosine similarity (1 for similar, 0 for not), are used for evaluation, with a threshold determining similarity.
"""

# Determines the threshold for categorizing similarity as 1 or 0
threshold = 0.5

# Create ground truth data with threshold assumptions
ground_truth = np.where(cosine_sim >= threshold, 1, 0)

# Displays several values in the ground truth matrix
ground_truth_df = pd.DataFrame(ground_truth, index=data['book_title'], columns=data['book_title']).sample(5, axis=1).sample(10, axis=0)

"""The code sets a threshold of 0.5, adjustable based on the recommendation results. A ground truth matrix is generated using NumPy's np.where() function, assigning 1 where cosine similarity meets or exceeds the threshold and 0 otherwise. The resulting matrix is presented as a dataframe, indexed by book titles."""

ground_truth_df

"""After creating the ground truth matrix, model evaluation is done using precision, recall, and F1 scores. The precision_recall_fscore_support function from Sklearn calculates these metrics. To speed up the process, only 10,000 samples are used, and the matrices are flattened into one-dimensional arrays. Cosine similarity values are categorized as 1 or 0 based on a threshold, and the results are stored in the predictions array. Precision, recall, and F1 scores are then computed with binary classification and zero division handling."""

# Takes a small portion of the cosine similarity matrix and ground truth matrix
sample_size = 10000
cosine_sim_sample = cosine_sim[:sample_size, :sample_size]
ground_truth_sample = ground_truth[:sample_size, :sample_size]

# Converts the cosine similarity matrix to a one-dimensional array for comparison
cosine_sim_flat = cosine_sim_sample.flatten()

# Converts the ground truth matrix into a one-dimensional array
ground_truth_flat = ground_truth_sample.flatten()

# Calculate evaluation metrics
predictions = (cosine_sim_flat >= threshold).astype(int)
precision, recall, f1, _ = precision_recall_fscore_support(
     ground_truth_flat, predictions, average='binary', zero_division=1
)

print("Precision:", precision)
print("Recall:", recall)
print("F1-score:", f1)

# prompt: plot confusion matrix

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix

# Assuming 'ground_truth_flat' and 'predictions' are defined as in the provided code
# Example:
# ground_truth_flat = np.array([0, 1, 1, 0, 1])
# predictions = np.array([1, 1, 0, 0, 1])


cm = confusion_matrix(ground_truth_flat, predictions)

plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
            xticklabels=['Predicted Negative', 'Predicted Positive'],
            yticklabels=['Actual Negative', 'Actual Positive'])
plt.title('Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

"""The evaluation results show that the model performs excellently, with a Precision of 1.0 (no false positives), a Recall of 1.0 (identifying nearly all relevant items), and an F1 Score close to 1.0, indicating a strong balance between precision and recall. These results demonstrate that the model is highly effective at recommending items using content-based filtering.

## Model Evaluation with Collaborative Filtering

In the Collaborative Filtering model, RMSE (Root Mean Squared Error) is used to evaluate how accurately the model predicts continuous values by comparing predicted and actual values. RMSE helps assess the model's ability to predict user preferences. The training results, including RMSE for both training and validation data, are plotted for visualization using matplotlib.
"""

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('Collaborative Filtering Model Performance')
plt.ylabel('RMSE')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper right')
plt.show()

"""The RMSE evaluation visualization shows that the model converges after about 50 epochs, achieving a low MSE value. The final error is 0.2924, with a validation error of 0.3389. These results indicate good performance, as a lower RMSE means better predictions of user preferences, making the recommendation system accurate."""
